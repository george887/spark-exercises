{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydataset import data\n",
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import month, year, quarter\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "- The name of the column should be language\n",
    "- View the schema of the dataframe\n",
    "- Output the shape of the dataframe\n",
    "- Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language\n",
       "0      python\n",
       "1        HTML\n",
       "2        java\n",
       "3         sql\n",
       "4  JavaScript"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The name of the column should be language\n",
    "df = pd.DataFrame({'language': ['python', 'HTML', 'java', 'sql','JavaScript']})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the spark session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[language: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the schema of the dataframe\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the schema of the dataframe\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the shape of the dataframe\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  language|\n",
      "+----------+\n",
      "|    python|\n",
      "|      HTML|\n",
      "|      java|\n",
      "|       sql|\n",
      "|JavaScript|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 records in the dataframe\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           pandas  \\\n",
       "csv     pd.read_csv(\"myfile.csv\")   \n",
       "json  pd.read_json(\"myfile.json\")   \n",
       "\n",
       "                                                                                  spark  \n",
       "csv                            spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json  spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd.DataFrame([['pd.read_csv(\"myfile.csv\")', \n",
    "                            'spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")'], \n",
    "                           ['pd.read_json(\"myfile.json\")', \n",
    "                            'spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")']], \n",
    "                          index = ['csv', 'json'], \n",
    "                          columns = ['pandas', 'spark'])\n",
    "\n",
    "# to display and see all text in dataframe\n",
    "pd.set_option('display.max_colwidth', 10000)\n",
    "\n",
    "\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          pandas  \\\n",
       "csv                    pd.read_csv(\"myfile.csv\")   \n",
       "json                 pd.read_json(\"myfile.json\")   \n",
       "1st n rows                          pd_df.head()   \n",
       "1st row                            pd_df.head(1)   \n",
       "summary statistics              pd_df.describe()   \n",
       "column names                       pd_df.columns   \n",
       "# rows                                len(pd_df)   \n",
       "# distinct rows     len(pd_df.drop_duplicates())   \n",
       "df schema info                      pd_df.info()   \n",
       "\n",
       "                                                                                                spark  \n",
       "csv                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                 sp_df.first()  \n",
       "summary statistics                                                                   sp_df.describe()  \n",
       "column names                                                                            sp_df.columns  \n",
       "# rows                                                                                  sp_df.count()  \n",
       "# distinct rows                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                    sp_df.printSchema()  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.head()', 'sp_df.show(), .head(), .take()'],\n",
    "                                             ['pd_df.head(1)', 'sp_df.first()'],\n",
    "                                             ['pd_df.describe()', 'sp_df.describe()'],\n",
    "                                             ['pd_df.columns', 'sp_df.columns'],\n",
    "                                             ['len(pd_df)', 'sp_df.count()'],\n",
    "                                             ['len(pd_df.drop_duplicates())', 'sp_df.distinct().count()'],\n",
    "                                             ['pd_df.info()', 'sp_df.printSchema()']\n",
    "                                            ],\n",
    "                                            index = ['1st n rows', '1st row','summary statistics', \n",
    "                                                     'column names', '# rows', '# distinct rows', \n",
    "                                                     'df schema info'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df[[\"col1\", \"col2\"]]', \n",
    "                                              'sp_df.select(sp_df.col1, sp_df.col2)']\n",
    "                                            ],\n",
    "                                            index = ['select columns'], \n",
    "                                            columns = ['pandas', 'spark']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         pandas  \\\n",
       "csv                                   pd.read_csv(\"myfile.csv\")   \n",
       "json                                pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                         pd_df.head()   \n",
       "1st row                                           pd_df.head(1)   \n",
       "summary statistics                             pd_df.describe()   \n",
       "column names                                      pd_df.columns   \n",
       "# rows                                               len(pd_df)   \n",
       "# distinct rows                    len(pd_df.drop_duplicates())   \n",
       "df schema info                                     pd_df.info()   \n",
       "select columns                          pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional assigning  np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "\n",
       "                                                                                                   spark  \n",
       "csv                                             spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                   spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                    sp_df.first()  \n",
       "summary statistics                                                                      sp_df.describe()  \n",
       "column names                                                                               sp_df.columns  \n",
       "# rows                                                                                     sp_df.count()  \n",
       "# distinct rows                                                                 sp_df.distinct().count()  \n",
       "df schema info                                                                       sp_df.printSchema()  \n",
       "select columns                                                      sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional assigning              sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"positive\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning with else</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"pos\", \"neg\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     pandas  \\\n",
       "csv                                               pd.read_csv(\"myfile.csv\")   \n",
       "json                                            pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                                     pd_df.head()   \n",
       "1st row                                                       pd_df.head(1)   \n",
       "summary statistics                                         pd_df.describe()   \n",
       "column names                                                  pd_df.columns   \n",
       "# rows                                                           len(pd_df)   \n",
       "# distinct rows                                len(pd_df.drop_duplicates())   \n",
       "df schema info                                                 pd_df.info()   \n",
       "select columns                                      pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional assigning              np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "conditional assigning with else  np.where(pd_df.c1.array > 0, \"pos\", \"neg\")   \n",
       "\n",
       "                                                                                                             spark  \n",
       "csv                                                       spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                             spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                          sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                              sp_df.first()  \n",
       "summary statistics                                                                                sp_df.describe()  \n",
       "column names                                                                                         sp_df.columns  \n",
       "# rows                                                                                               sp_df.count()  \n",
       "# distinct rows                                                                           sp_df.distinct().count()  \n",
       "df schema info                                                                                 sp_df.printSchema()  \n",
       "select columns                                                                sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional assigning                        sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  \n",
       "conditional assigning with else  sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['np.where(pd_df.c1.array > 0, \"pos\", \"neg\")', \n",
    "                                              'sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))'],\n",
    "                                            ],\n",
    "                                            index = ['conditional assigning with else'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pandas</th>\n",
       "      <th>spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>csv</th>\n",
       "      <td>pd.read_csv(\"myfile.csv\")</td>\n",
       "      <td>spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>json</th>\n",
       "      <td>pd.read_json(\"myfile.json\")</td>\n",
       "      <td>spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st n rows</th>\n",
       "      <td>pd_df.head()</td>\n",
       "      <td>sp_df.show(), .head(), .take()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st row</th>\n",
       "      <td>pd_df.head(1)</td>\n",
       "      <td>sp_df.first()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary statistics</th>\n",
       "      <td>pd_df.describe()</td>\n",
       "      <td>sp_df.describe()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column names</th>\n",
       "      <td>pd_df.columns</td>\n",
       "      <td>sp_df.columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># rows</th>\n",
       "      <td>len(pd_df)</td>\n",
       "      <td>sp_df.count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># distinct rows</th>\n",
       "      <td>len(pd_df.drop_duplicates())</td>\n",
       "      <td>sp_df.distinct().count()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df schema info</th>\n",
       "      <td>pd_df.info()</td>\n",
       "      <td>sp_df.printSchema()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select columns</th>\n",
       "      <td>pd_df[[\"col1\", \"col2\"]]</td>\n",
       "      <td>sp_df.select(sp_df.col1, sp_df.col2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"positive\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"positive\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conditional assigning with else</th>\n",
       "      <td>np.where(pd_df.c1.array &gt; 0, \"pos\", \"neg\")</td>\n",
       "      <td>sp_df.select(df.c1, when(df.c1 &gt; 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 1 col asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"])</td>\n",
       "      <td>sp_df.sort(sp_df.c1, sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc/asc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])</td>\n",
       "      <td>sp_df.sort(sp_df.c1.desc(), sp_df.c2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort 2+ cols desc</th>\n",
       "      <td>pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)</td>\n",
       "      <td>sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     pandas  \\\n",
       "csv                                                               pd.read_csv(\"myfile.csv\")   \n",
       "json                                                            pd.read_json(\"myfile.json\")   \n",
       "1st n rows                                                                     pd_df.head()   \n",
       "1st row                                                                       pd_df.head(1)   \n",
       "summary statistics                                                         pd_df.describe()   \n",
       "column names                                                                  pd_df.columns   \n",
       "# rows                                                                           len(pd_df)   \n",
       "# distinct rows                                                len(pd_df.drop_duplicates())   \n",
       "df schema info                                                                 pd_df.info()   \n",
       "select columns                                                      pd_df[[\"col1\", \"col2\"]]   \n",
       "conditional assigning                              np.where(pd_df.c1.array > 0, \"positive\")   \n",
       "conditional assigning with else                  np.where(pd_df.c1.array > 0, \"pos\", \"neg\")   \n",
       "sort 1 col asc                                                 pd_df.sort_values(by=[\"c1\"])   \n",
       "sort 2+ cols asc                                          pd_df.sort_values(by=[\"c1\",\"c2\"])   \n",
       "sort 2+ cols desc/asc            pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])   \n",
       "sort 2+ cols desc                        pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)   \n",
       "\n",
       "                                                                                                                spark  \n",
       "csv                                                          spark.read.load(\"myfile.csv\", format = \"csv\", sep = \",\")  \n",
       "json                                spark.read.load(\"myfile.json\", format = \"json\") OR spark.read.json(\"myfile.json\")  \n",
       "1st n rows                                                                             sp_df.show(), .head(), .take()  \n",
       "1st row                                                                                                 sp_df.first()  \n",
       "summary statistics                                                                                   sp_df.describe()  \n",
       "column names                                                                                            sp_df.columns  \n",
       "# rows                                                                                                  sp_df.count()  \n",
       "# distinct rows                                                                              sp_df.distinct().count()  \n",
       "df schema info                                                                                    sp_df.printSchema()  \n",
       "select columns                                                                   sp_df.select(sp_df.col1, sp_df.col2)  \n",
       "conditional assigning                           sp_df.select(df.c1, when(df.c1 > 0, \"positive\").alias(\"number_sign\"))  \n",
       "conditional assigning with else     sp_df.select(df.c1, when(df.c1 > 0, \"pos\").otherwise(\"neg\").alias(\"number_sign\"))  \n",
       "sort 1 col asc                                                                                   sp_df.sort(sp_df.c1)  \n",
       "sort 2+ cols asc                                                                       sp_df.sort(sp_df.c1, sp_df.c2)  \n",
       "sort 2+ cols desc/asc                                                           sp_df.sort(sp_df.c1.desc(), sp_df.c2)  \n",
       "sort 2+ cols desc                sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_v_spark = pd_v_spark.append(pd.DataFrame([['pd_df.sort_values(by=[\"c1\"])', \n",
    "                                              'sp_df.sort(sp_df.c1)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"])',\n",
    "                                              'sp_df.sort(sp_df.c1, sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=[False, True])',\n",
    "                                              'sp_df.sort(sp_df.c1.desc(), sp_df.c2)'],\n",
    "                                             ['pd_df.sort_values(by=[\"c1\",\"c2\"], ascending=False)', \n",
    "                                              'sp_df.sort(desc(\"c1\"), desc(\"c2\")) OR sp_df.sort(col(\"c1\").desc(), col(\"c2\").desc())']\n",
    "                                            ],\n",
    "                                            index = ['sort 1 col asc', 'sort 2+ cols asc', 'sort 2+ cols desc/asc', 'sort 2+ cols desc'], \n",
    "                                            columns = ['pandas', 'spark']))\n",
    "pd_v_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the mpg dataset as a spark dataframe.\n",
    "\n",
    "A. Create 1 column of output that contains a message like the one below:\n",
    "\n",
    "```The 1999 audi a4 has a 4 cylinder engine.```\n",
    "\n",
    "For each vehicle.\n",
    "\n",
    "B. Transform the trans column so that it only contains either manual or auto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>displ</th>\n",
       "      <th>year</th>\n",
       "      <th>cyl</th>\n",
       "      <th>trans</th>\n",
       "      <th>drv</th>\n",
       "      <th>cty</th>\n",
       "      <th>hwy</th>\n",
       "      <th>fl</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m5)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m6)</td>\n",
       "      <td>f</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(av)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>6</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manufacturer model  displ  year  cyl       trans drv  cty  hwy fl    class\n",
       "1         audi    a4    1.8  1999    4    auto(l5)   f   18   29  p  compact\n",
       "2         audi    a4    1.8  1999    4  manual(m5)   f   21   29  p  compact\n",
       "3         audi    a4    2.0  2008    4  manual(m6)   f   20   31  p  compact\n",
       "4         audi    a4    2.0  2008    4    auto(av)   f   21   30  p  compact\n",
       "5         audi    a4    2.8  1999    6    auto(l5)   f   16   26  p  compact"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas df\n",
    "mpg_pd = data('mpg')\n",
    "mpg_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating spark DF\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|hwy|cty|model|\n",
      "+---+---+-----+\n",
      "| 29| 18|   a4|\n",
      "| 29| 21|   a4|\n",
      "| 31| 20|   a4|\n",
      "| 30| 21|   a4|\n",
      "| 26| 16|   a4|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Create 1 column of output that contains a message like the one below:\n",
    "```The 1999 audi a4 has a 4 cylinder engine.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|Car Description                       |\n",
      "+--------------------------------------+\n",
      "|The 1999 audi has a 4 cylinder engine.|\n",
      "|The 1999 audi has a 4 cylinder engine.|\n",
      "|The 2008 audi has a 4 cylinder engine.|\n",
      "|The 2008 audi has a 4 cylinder engine.|\n",
      "|The 1999 audi has a 6 cylinder engine.|\n",
      "+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Do one item at a time to see how it prints out!\n",
    "mpg.select(concat(lit(\"The \"), mpg.year, lit(\" \"), mpg.manufacturer, \n",
    "                  lit(\" \"), lit(\"has a \"), mpg.cyl, lit(\" cylinder engine.\"))\n",
    "           .alias('Car Description')).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Transform the trans column so that it only contains either manual or auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+----+---+--------+---+---+---+---+-------+\n",
      "|manufacturer| model|displ|year|cyl|   trans|drv|cty|hwy| fl|  class|\n",
      "+------------+------+-----+----+---+--------+---+---+---+---+-------+\n",
      "|      nissan|maxima|  3.5|2008|  6|auto(av)|  f| 19| 25|  p|midsize|\n",
      "|        audi|    a4|  2.0|2008|  4|auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|    a4|  3.1|2008|  6|auto(av)|  f| 18| 27|  p|compact|\n",
      "+------------+------+-----+----+---+--------+---+---+---+---+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sp_df.sort(sp_df.c1, sp_df.c2) Sorting by transmission\n",
    "mpg.sort(mpg.trans).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+--------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|   trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+--------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+--------+---+---+---+---+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtered by trans\n",
    "mpg.filter(mpg['trans'].like('%auto%')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     trans|manual|\n",
      "+----------+------+\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|manual(m6)|manual|\n",
      "|  auto(av)|  auto|\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|  auto(av)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m6)|manual|\n",
      "|  auto(s6)|  auto|\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|  auto(s6)|  auto|\n",
      "|manual(m6)|manual|\n",
      "|  auto(l5)|  auto|\n",
      "|  auto(s6)|  auto|\n",
      "|  auto(s6)|  auto|\n",
      "|  auto(l4)|  auto|\n",
      "|  auto(l4)|  auto|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    'trans',\n",
    "    regexp_extract('trans', r\"^(\\w+)\",1).alias('manual')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl| trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto|  f| 16| 26|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|manual|  f| 18| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing all the pyspark equations\n",
    "from pyspark.sql.functions import *\n",
    "mpg.withColumn(\"trans\", when(mpg.trans.startswith(\"a\"), \"auto\").otherwise(\"manual\")).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the tips dataset as a spark dataframe.\n",
    "\n",
    "A. What percentage of observations are smokers?\n",
    "\n",
    "B. Create a column that contains the tip percentage\n",
    "\n",
    "C. Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating spark DF\n",
    "tips = spark.createDataFrame(data(\"tips\"))\n",
    "tips.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. What percentage of observations are smokers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|smoker|smoker_percentages|\n",
      "+------+------------------+\n",
      "|    No|              0.62|\n",
      "|   Yes|              0.38|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.groupBy('smoker').agg(round(count(tips.smoker)/ tips.count(),2).alias('smoker_percentages')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Create a column that contains the tip percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| (tip / total_bill)|\n",
      "+-------------------+\n",
      "|0.05944673337257211|\n",
      "|0.16054158607350097|\n",
      "|0.16658733936220846|\n",
      "| 0.1397804054054054|\n",
      "|0.14680764538430255|\n",
      "|0.18623962040332148|\n",
      "|0.22805017103762829|\n",
      "|0.11607142857142858|\n",
      "|0.13031914893617022|\n",
      "| 0.2185385656292287|\n",
      "| 0.1665043816942551|\n",
      "|0.14180374361883155|\n",
      "|0.10181582360570687|\n",
      "|0.16277807921866522|\n",
      "|0.20364126770060686|\n",
      "|0.18164967562557924|\n",
      "| 0.1616650532429816|\n",
      "|0.22774708410067526|\n",
      "|0.20624631703005306|\n",
      "|0.16222760290556903|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tip_percentage = tips.select(tips.tip/tips.total_bill).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+--------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|tip_percentage|\n",
      "+----------+----+------+------+---+------+----+--------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|           6.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|          16.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|          17.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|          14.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|          15.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|          19.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|          23.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|          12.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|          13.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|          22.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|          17.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|          14.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|          10.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|          16.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|          20.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|          18.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|          16.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|          23.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|          21.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|          16.0|\n",
      "+----------+----+------+------+---+------+----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips = tips.withColumn('tip_percentage', expr('Round((tip/total_bill) * 100)'))\n",
    "tips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|   sex|smoker|round(avg(tip), 2)|\n",
      "+------+------+------------------+\n",
      "|  Male|    No|              3.11|\n",
      "|  Male|   Yes|              3.05|\n",
      "|Female|    No|              2.77|\n",
      "|Female|   Yes|              2.93|\n",
      "+------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.groupBy(tips.sex,tips.smoker).agg(round(mean(tips.tip),2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "- Convert the temperatures to farenheight.\n",
    "- Which month has the most rain, on average?\n",
    "- Which year was the windiest?\n",
    "- What is the most frequent type of weather in January?\n",
    "- What is the average high and low temperature on sunny days in July in 2013 and 2014?\n",
    "- What percentage of days were rainy in q3 of 2015?\n",
    "- For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0847bb7f7257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvega_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseattle_weather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weather\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    603\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    604\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    381\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    382\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    381\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    382\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(\"weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
